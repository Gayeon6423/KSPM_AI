# 1. 배경 & 개요

---

- 주제명 : 주식 매매를 위한 Policy-based Reinforce Agent 개발
- 배경
    - 딥러닝을 이용하여 주식 가격을 예측하는 프로젝트는 일반적으로 시도되었으나, 단순 예측에 기반하기 때문에 투자 의사결정 및 수익률을 극대화하는 주식 매매의 목적을 이루기에 한계가 있음
    - 불확실한 환경에서 매매를 해야하는 현실의 문제에 맞도록 에이전트가 수 십만개의 시나리오를 통해 수익률을 극대화할 수 있는 액션을 학습하게하여 수익률을 극대화하고자 함
- 개요 : 강화학습 에이전트가 수익률을 극대화하는 거래를 할 수 있도록 학습하고 KOSPI 200 종목에서 실전 매매를 진행

# 2. 주최 & 참가 대상 & 성과

---

- 주최 : 데이터분석학회 D&A Conference Session
- 참가 자격 및 팀 인원 제한 사항 : 3인 팀
- 성과 : 컨퍼런스 발표

# 3. 프로젝트 기간

---

- 프로젝트 기간 : 2023년 7월~2023년 11월
- 컨퍼런스 발표 : 2023년 12월 1일

# 4. 담당 역할

---

- 주가 데이터 및 재무제표 데이터 수집
- 주가 데이터 전처리
- 퀄리티 팩터 투자 구현
- 강화 학습 이론 정리 및 실험 수행

# 5. 방법론 & 개발 과정

---

### **방법론 : Advantage Actor Critic**

- 정책 경사를 활용하여 Policy Network의 액션(Buy, Sell, Hold) 학습
- Critic Network 도입으로 Policy Network가 예측한 action의 가치 측정
- Advantage Term (Q(st,at)- V(st))를 이용하여 Policy, Critic Network 학습
- A2C는 Onpolicy이므로 N개에 대한 Batch 학습을 진행하고 샘플을 재사용하지 않는다
- 보상함수 설계
    - one buy - one sell이 일어날 시 매도 차익에 대한 수익률로 보상 설정
    - Monte carlo, Target Difference 방식 중 주식 거래 도메인에 더 효과적인 방식 채택

  ![주식매매png](https://github.com/user-attachments/assets/fef1a711-2b6b-42ad-80d0-93f7d6b1e5d1)
    

## **개발 과정**

### **데이터 및 처리**

- KOSPI 200에 해당하는 종목의 지난 10년(2013.01.01~2022.12.31) 일별 데이터를 수집(2023.08.26 기준)
    - 수집 데이터 : 시가, 고가, 저가, 종가, 거래량, 투자자별 거래대금(기관, 개인, 외국인)
- 거래량 및 가격 지표를 생성 및 전처리(클렌징, 변수변환 등)
- 학습기간 동안 사용한 스케일러를 테스트 때 사용하기 위해서 종목별 스케일러 저장

### **환경 구성**

- Environment와 state를 구성하기 위한 작업 실시
    - 개별 종목 코드를 부르면 전체 주가 데이터를 불러오는 기능
    - 환경에는 두 가지 데이터 셋이 존재
        - 차트 데이터 : 에이전트가 실거래가로 매매하기 위해 사용 (시가, 저가, 고가, 종가)
        - 학습 데이터 : policy network에 들어가기 위한 데이터 (각종 지표들)
    - 지정한 윈도우 사이즈만큼 개별 state를 호출하고 next state를 불러주는 기능

### **에이전트**

- 거래 관련 파라미터
    - 운용자금, 포트폴리오 가치, 평단가, 거래 수수료 등
- 거래 관련 함수 선언
    - Buy, Sell, Hold 등
- Policy Network (Gradient Ascent)
    - loss = -logprob * advantage
- Critic Network (Gradient Descent)
    - loss = (advantage)^2
- Advantage = V(st+1) - V(st) :
    - Q(st,at)를 bellman equation으로 변환 후 state만을 가지고 advantage term을 나타낼 수 있어 연산 효율화 가능

# 6. 느낀점

---

### 의의

- 전통적인 금융공학 분야에서 쓰이는 알고리즘과 달리, 스스로의 경험으로 학습하는 인공지능을 개발하는데 의미가 있었습니다.
- 강화학습 기반의 알고리즘에 대한 연구가 활성화 되어있지 않았기 때문에 새롭고 도전적인 연구라고 생각합니다.
- 프레임워크를 사용하지 않고 직접 End to End 모델을 개발하며 값진 경험을 얻었다는데 큰 의미를 가집니다.

### 한계 및 보완점

- 통제할 하이퍼파라미터 증가로 인해 최적의 모델을 탐색하는데 어려움을 겪었습니다.
- 하락장에서는 거의 완패하면서 수익률을 보지 못하는 경우가 다수였고 변동성이 클 때 제대로 된 액션을 하지 못하는 현상이 발생했습니다.
- 보상함수의 보완을 통해 모델이 정교한 학습을 하도록 유도가 필요하다고 생각합니다.
- 모델의 액션에만 의존하는 것이 아닌 금융공학적인 규칙을 주입하여 액션을 조정하는 방안이 필요할 것으로 보입니다. (Ex. 손절 하한선 설정, N회 이상 매수 금지 등)
